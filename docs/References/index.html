<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>References - LLMIR</title>
<meta name=description content="Large Language Model IR Compiler Framework"><meta name=generator content="Hugo 0.129.0"><link href=https://chenxingqiang.github.io/llmir-www/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://chenxingqiang.github.io/llmir-www/docs/References/><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://chenxingqiang.github.io/llmir-www/js/bundle.js></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://chenxingqiang.github.io/llmir-www//mlir-logo.png width=40px align=absmiddle>
LLMIR</div></h1><span class=version>Version 0.0.1</span><p class=description>Large Language Model IR Compiler Framework</p></header><div class=global-menu><nav><ul><li><a href=/llmir-www/governance/>Governance</a></li><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/llmir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/llmir-www/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/chenxingqiang/llmir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/llmir-www/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/chenxingqiang/llmir>GitHub</a></li></ul></li><li><a href=https://github.com/chenxingqiang/llmir/issues>Bugs</a></li><li><a href=https://github.com/chenxingqiang/llmir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/LLMIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>References</h1><h1 id=references>References</h1><p>This page lists relevant papers, projects, and resources that have influenced the development of LLMIR or are related to LLM optimization and compilation.</p><h2 id=foundational-work>Foundational Work&nbsp;<a class=headline-hash href=#foundational-work>¶</a></h2><h3 id=mlir-framework>MLIR Framework&nbsp;<a class=headline-hash href=#mlir-framework>¶</a></h3><ul><li>Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and Oleksandr Zinenko. <strong>&ldquo;MLIR: Scaling compiler infrastructure for domain specific computation.&rdquo;</strong> In 2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO), pp. 2-14. IEEE, 2021.
<a href=https://ieeexplore.ieee.org/abstract/document/9370308>Link</a></li></ul><h3 id=large-language-model-inference>Large Language Model Inference&nbsp;<a class=headline-hash href=#large-language-model-inference>¶</a></h3><ul><li><p>Dao, Tri, et al. <strong>&ldquo;FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.&rdquo;</strong> Advances in Neural Information Processing Systems, 2022.
<a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper.pdf>Link</a></p></li><li><p>Sheng, Ying, et al. <strong>&ldquo;High-throughput Generative Inference of Large Language Models with a Single GPU.&rdquo;</strong> In International Conference on Machine Learning, 2023.
<a href=https://arxiv.org/abs/2303.06865>Link</a></p></li></ul><h2 id=llm-inference-frameworks>LLM Inference Frameworks&nbsp;<a class=headline-hash href=#llm-inference-frameworks>¶</a></h2><h3 id=vllm>vLLM&nbsp;<a class=headline-hash href=#vllm>¶</a></h3><ul><li>Kwon, Woosuk, et al. <strong>&ldquo;Efficient Memory Management for Large Language Model Serving with PagedAttention.&rdquo;</strong> In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023.
<a href=https://arxiv.org/abs/2309.06180>Link</a></li></ul><h3 id=sglang>SGLang&nbsp;<a class=headline-hash href=#sglang>¶</a></h3><ul><li>Xiao, Lianmin, et al. <strong>&ldquo;SGLang: Semi-Structured Gateway Language for Multi-Agent System.&rdquo;</strong> arXiv preprint arXiv:2403.06071 (2024).
<a href=https://arxiv.org/abs/2403.06071>Link</a></li></ul><h2 id=compiler-optimization-for-llms>Compiler Optimization for LLMs&nbsp;<a class=headline-hash href=#compiler-optimization-for-llms>¶</a></h2><ul><li><p>DeepSpeed Team. <strong>&ldquo;DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale.&rdquo;</strong> arXiv preprint arXiv:2207.00032 (2022).
<a href=https://arxiv.org/abs/2207.00032>Link</a></p></li><li><p>Frantar, Elias, et al. <strong>&ldquo;GPTQ: Accurate Post-training Quantization for Generative Pre-trained Transformers.&rdquo;</strong> In International Conference on Learning Representations, 2023.
<a href=https://arxiv.org/abs/2210.17323>Link</a></p></li><li><p>Korthikanti, Vijay Anand, et al. <strong>&ldquo;Reducing Activation Recomputation in Large Transformer Models.&rdquo;</strong> In Proceedings of Machine Learning and Systems, 2023.
<a href=https://proceedings.mlsys.org/paper_files/paper/2023/file/9fb11ca5c4611e9a545e15f04bea4afd-Paper-Conference.pdf>Link</a></p></li></ul><h2 id=hardware-specific-optimizations>Hardware-Specific Optimizations&nbsp;<a class=headline-hash href=#hardware-specific-optimizations>¶</a></h2><ul><li><p>Wang, Ze, et al. <strong>&ldquo;TensorRT-LLM: A Comprehensive and Efficient Large Language Model Inference Library.&rdquo;</strong> arXiv preprint arXiv:2405.09386 (2024).
<a href=https://arxiv.org/abs/2405.09386>Link</a></p></li><li><p>Dao, Tri, et al. <strong>&ldquo;FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.&rdquo;</strong> arXiv preprint arXiv:2307.08691 (2023).
<a href=https://arxiv.org/abs/2307.08691>Link</a></p></li></ul><h2 id=quantization-techniques>Quantization Techniques&nbsp;<a class=headline-hash href=#quantization-techniques>¶</a></h2><ul><li><p>Dettmers, Tim, et al. <strong>&ldquo;LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale.&rdquo;</strong> In Advances in Neural Information Processing Systems, 2022.
<a href=https://proceedings.neurips.cc/paper_files/paper/2022/file/721d1c440afcb96283fb84384663a667-Paper-Conference.pdf>Link</a></p></li><li><p>Xiao, Guangxuan, et al. <strong>&ldquo;SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models.&rdquo;</strong> In Proceedings of the International Conference on Machine Learning, 2023.
<a href=https://arxiv.org/abs/2211.10438>Link</a></p></li></ul><h2 id=distributed-llm-inference>Distributed LLM Inference&nbsp;<a class=headline-hash href=#distributed-llm-inference>¶</a></h2><ul><li><p>Aminabadi, Reza Yazdani, et al. <strong>&ldquo;DeepSpeed-Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale.&rdquo;</strong> In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 2022.
<a href=https://arxiv.org/abs/2207.00032>Link</a></p></li><li><p>Zheng, Lianmin, et al. <strong>&ldquo;Alpa: Automating Inter-and Intra-Operator Parallelism for Distributed Deep Learning.&rdquo;</strong> In 16th USENIX Symposium on Operating Systems Design and Implementation, 2022.
<a href=https://arxiv.org/abs/2201.12023>Link</a></p></li></ul><h2 id=related-projects>Related Projects&nbsp;<a class=headline-hash href=#related-projects>¶</a></h2><ul><li><a href=https://llvm.org/>LLVM</a>: The LLVM Compiler Infrastructure Project</li><li><a href=https://mlir.llvm.org/>MLIR</a>: Multi-Level Intermediate Representation</li><li><a href=https://github.com/vllm-project/vllm>vLLM</a>: High-throughput and memory-efficient inference and serving engine for LLMs</li><li><a href=https://github.com/sgl-project/sglang>SGLang</a>: Semi-structured gateway language for large language models</li><li><a href=https://github.com/NVIDIA/TensorRT-LLM>TensorRT-LLM</a>: NVIDIA&rsquo;s LLM optimization library</li></ul><hr><p>This reference list will be updated as the LLMIR project progresses and new relevant research emerges in the field.</p><div class=edit-meta><br><a href=https://github.com/chenxingqiang/llmir-www//edit/main/docs/content/ class=edit-page><i class="fas fa-pen-square"></i> Edit on GitHub</a></div><nav class=pagination><a class="nav nav-prev" href=https://chenxingqiang.github.io/llmir-www/docs/architecture/PerformanceEvaluation/ title="Performance Evaluation"><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - Performance Evaluation</a>
<a class="nav nav-next" href=https://chenxingqiang.github.io/llmir-www/pubs/ title="LLMIR Related Publications">Next - LLMIR Related Publications <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://chenxingqiang.github.io/llmir-www/>Home</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/governance/>Governance</a></li><li class="parent has-sub-menu"><a href=https://chenxingqiang.github.io/llmir-www/docs/>Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class=has-sub-menu><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/>Architecture<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/>KV Cache Optimization</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/Quantization/>Quantization Support</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/DistributedDeployment/>Distributed Deployment</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/PerformanceEvaluation/>Performance Evaluation</a></li></ul></li><li class=active><a href=https://chenxingqiang.github.io/llmir-www/docs/References/>References</a></li></ul></li><li><a href=https://chenxingqiang.github.io/llmir-www/pubs/>LLMIR Related Publications</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/users/>Users of MLIR</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/deprecation/>Deprecations & Current Refactoring</a></li><li class=has-sub-menu><a href=https://chenxingqiang.github.io/llmir-www/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Faq/>FAQ</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/openprojects/>Open Projects</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Glossary/>Glossary</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/TestingGuide/>Testing Guide</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/DeveloperGuide/>Developer Guide</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>