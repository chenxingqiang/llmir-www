<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>KV Cache Optimization - LLMIR</title>
<meta name=description content="Large Language Model IR Compiler Framework"><meta name=generator content="Hugo 0.129.0"><link href=https://chenxingqiang.github.io/llmir-www/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://chenxingqiang.github.io/llmir-www/js/bundle.js></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://chenxingqiang.github.io/llmir-www//mlir-logo.png width=40px align=absmiddle>
LLMIR</div></h1><span class=version>Version 0.0.1</span><p class=description>Large Language Model IR Compiler Framework</p></header><div class=global-menu><nav><ul><li><a href=/llmir-www/governance/>Governance</a></li><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/llmir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/llmir-www/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/chenxingqiang/llmir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/llmir-www/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/chenxingqiang/llmir>GitHub</a></li></ul></li><li><a href=https://github.com/chenxingqiang/llmir/issues>Bugs</a></li><li><a href=https://github.com/chenxingqiang/llmir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/LLMIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>KV Cache Optimization</h1><h1 id=kv-cache-optimization-in-llmir>KV Cache Optimization in LLMIR</h1><p>KV cache management is one of the core optimizations in LLMIR, focusing on efficient handling of key-value pairs in attention mechanisms for transformer-based LLMs.</p><h2 id=the-kv-cache-challenge>The KV Cache Challenge&nbsp;<a class=headline-hash href=#the-kv-cache-challenge>¶</a></h2><p>In large language model inference, the key-value cache stores computed key and value tensors from previous tokens to avoid redundant computation. As sequence lengths grow, efficiently managing this cache becomes critical for:</p><ul><li>Memory efficiency</li><li>Computation speed</li><li>Support for longer contexts</li><li>Dynamic batch handling</li></ul><h2 id=pagedattention-in-llmir>PagedAttention in LLMIR&nbsp;<a class=headline-hash href=#pagedattention-in-llmir>¶</a></h2><p>LLMIR implements a paged attention mechanism inspired by vLLM&rsquo;s approach, which treats the KV cache as blocks of memory rather than a continuous buffer:</p><h3 id=block-based-kv-cache>Block-based KV Cache&nbsp;<a class=headline-hash href=#block-based-kv-cache>¶</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=c>// Define a paged KV cache type
</span></span></span><span class=line><span class=cl><span class=c></span><span class=p>!</span><span class=nl>kv_cache_t =</span> <span class=p>!</span>llm<span class=p>.</span>paged_kv_cache<span class=p>&lt;</span><span class=k>f16</span><span class=p>,</span> <span class=m>12</span><span class=p>,</span> <span class=m>16</span><span class=p>,</span> <span class=m>64</span><span class=p>,</span> <span class=m>16</span><span class=p>,</span> <span class=m>4096</span><span class=p>&gt;</span>
</span></span></code></pre></div><p>Key parameters:</p><ul><li>Element type: <code>f16</code></li><li>Number of layers: <code>12</code></li><li>Number of heads: <code>16</code></li><li>Head dimension: <code>64</code></li><li>Block size: <code>16</code> (tokens per block)</li><li>Maximum sequence length: <code>4096</code></li></ul><h3 id=key-cache-operations>Key Cache Operations&nbsp;<a class=headline-hash href=#key-cache-operations>¶</a></h3><p>LLMIR provides specialized operations for managing the KV cache:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=c>// Append new key-value pairs to the cache
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%new_kv</span><span class=p>,</span> <span class=nv>%block_indices</span> <span class=p>=</span> llm<span class=p>.</span>append_kv <span class=nv>%kv_cache</span><span class=p>,</span> <span class=nv>%keys</span><span class=p>,</span> <span class=nv>%values</span><span class=p>,</span> <span class=nv>%seq_ids</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>block_size =</span> <span class=m>16</span> <span class=p>:</span> <span class=k>i32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nl>max_seq_len =</span> <span class=m>4096</span> <span class=p>:</span> <span class=k>i32</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=p>:</span> <span class=p>(!</span>kv_cache_t<span class=p>,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x1x16x64x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x1x16x64x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i32</span><span class=p>&gt;)</span> 
</span></span><span class=line><span class=cl>    <span class=p>-&gt;</span> <span class=p>(!</span>kv_cache_t<span class=p>,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x1x</span><span class=k>i32</span><span class=p>&gt;)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// Perform paged attention with the KV cache
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%output</span> <span class=p>=</span> llm<span class=p>.</span>paged_attention <span class=nv>%query</span><span class=p>,</span> <span class=nv>%new_kv</span><span class=p>,</span> <span class=nv>%block_indices</span><span class=p>,</span> <span class=nv>%seq_lens</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>num_heads =</span> <span class=m>16</span> <span class=p>:</span> <span class=k>i32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nl>head_dim =</span> <span class=m>64</span> <span class=p>:</span> <span class=k>i32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nl>scale =</span> <span class=m>0.125</span> <span class=p>:</span> <span class=k>f32</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x1x16x64x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=p>!</span>kv_cache_t<span class=p>,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x128x</span><span class=k>i32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x</span><span class=k>i32</span><span class=p>&gt;)</span> 
</span></span><span class=line><span class=cl>    <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>2x1x16x64x</span><span class=k>f16</span><span class=p>&gt;</span>
</span></span></code></pre></div><h2 id=runtime-implementation>Runtime Implementation&nbsp;<a class=headline-hash href=#runtime-implementation>¶</a></h2><p>The LLMIR runtime library will include an efficient implementation of the paged KV cache:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// C++ Runtime API (Planned)
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>class</span> <span class=nc>PagedKVCache</span> <span class=p>{</span>
</span></span><span class=line><span class=cl><span class=k>public</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=n>PagedKVCache</span><span class=p>(</span><span class=kt>int</span> <span class=n>numLayers</span><span class=p>,</span> <span class=kt>int</span> <span class=n>numHeads</span><span class=p>,</span> <span class=kt>int</span> <span class=n>headDim</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>               <span class=kt>int</span> <span class=n>blockSize</span><span class=p>,</span> <span class=kt>int</span> <span class=n>maxSeqLen</span><span class=p>,</span> <span class=n>ElementType</span> <span class=n>type</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1>// Append new KV pairs to the cache
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>void</span> <span class=nf>appendKV</span><span class=p>(</span><span class=kt>void</span><span class=o>*</span> <span class=n>keyPtr</span><span class=p>,</span> <span class=kt>void</span><span class=o>*</span> <span class=n>valuePtr</span><span class=p>,</span> <span class=kt>int</span> <span class=n>batchSize</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>                <span class=kt>int</span> <span class=n>seqLen</span><span class=p>,</span> <span class=kt>int</span><span class=o>*</span> <span class=n>seqIds</span><span class=p>,</span> <span class=kt>int</span><span class=o>*</span> <span class=n>blockIndices</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl>  <span class=c1>// Lookup existing KV pairs
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=kt>void</span> <span class=nf>lookupKV</span><span class=p>(</span><span class=kt>int</span><span class=o>*</span> <span class=n>blockIndices</span><span class=p>,</span> <span class=kt>int</span><span class=o>*</span> <span class=n>seqLens</span><span class=p>,</span> <span class=kt>int</span> <span class=n>batchSize</span><span class=p>,</span>
</span></span><span class=line><span class=cl>                <span class=kt>void</span><span class=o>*</span> <span class=n>outputKeys</span><span class=p>,</span> <span class=kt>void</span><span class=o>*</span> <span class=n>outputValues</span><span class=p>);</span>
</span></span><span class=line><span class=cl>  
</span></span><span class=line><span class=cl><span class=k>private</span><span class=o>:</span>
</span></span><span class=line><span class=cl>  <span class=c1>// Block-based memory manager
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>BlockAllocator</span> <span class=n>allocator_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=c1>// Maps sequence IDs to block indices
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=n>std</span><span class=o>::</span><span class=n>unordered_map</span><span class=o>&lt;</span><span class=kt>int</span><span class=p>,</span> <span class=n>std</span><span class=o>::</span><span class=n>vector</span><span class=o>&lt;</span><span class=kt>int</span><span class=o>&gt;&gt;</span> <span class=n>seqToBlocks_</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=c1>// ... other implementation details
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>};</span>
</span></span></code></pre></div><h2 id=optimization-passes>Optimization Passes&nbsp;<a class=headline-hash href=#optimization-passes>¶</a></h2><p>LLMIR will include several optimization passes for the KV cache:</p><ol><li><strong>BlockifyKVCachePass</strong>: Convert continuous KV caches to block-based representations</li><li><strong>PagedAttentionRewritePass</strong>: Rewrite standard attention to use paged attention</li><li><strong>KVCacheAllocationOptimizationPass</strong>: Optimize memory allocation for KV caches</li><li><strong>KVCachePruningPass</strong>: Remove unused or stale entries in the KV cache</li><li><strong>KVCacheShardingPass</strong>: Support sharded KV caches for large models</li></ol><h2 id=memory-management>Memory Management&nbsp;<a class=headline-hash href=#memory-management>¶</a></h2><p>The KV cache implementation uses block-based memory management to:</p><ul><li>Allocate memory in fixed-size blocks</li><li>Efficiently handle varying sequence lengths</li><li>Minimize memory fragmentation</li><li>Enable fast memory reuse for finished sequences</li></ul><h2 id=future-enhancements>Future Enhancements&nbsp;<a class=headline-hash href=#future-enhancements>¶</a></h2><p>As part of LLMIR&rsquo;s roadmap, the KV cache optimization will be enhanced with:</p><ul><li>Support for multi-head attention variants</li><li>Quantized KV cache for reduced memory footprint</li><li>Distributed KV cache for multi-device inference</li><li>Cache eviction policies for memory-constrained environments</li></ul><p>This feature is currently under development as part of Phase 2 of the LLMIR project.</p><div class=edit-meta><br><a href=https://github.com/chenxingqiang/llmir-www//edit/main/docs/content/ class=edit-page><i class="fas fa-pen-square"></i> Edit on GitHub</a></div><nav class=pagination><a class="nav nav-prev" href=https://chenxingqiang.github.io/llmir-www/docs/architecture/ title=Architecture><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - Architecture</a>
<a class="nav nav-next" href=https://chenxingqiang.github.io/llmir-www/docs/architecture/Quantization/ title="Quantization Support">Next - Quantization Support <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://chenxingqiang.github.io/llmir-www/>Home</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/governance/>Governance</a></li><li class="parent has-sub-menu"><a href=https://chenxingqiang.github.io/llmir-www/docs/>Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class="parent has-sub-menu"><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/>Architecture<span class="mark opened">-</span></a><ul class=sub-menu><li class=active><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/>KV Cache Optimization</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/Quantization/>Quantization Support</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/DistributedDeployment/>Distributed Deployment</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/PerformanceEvaluation/>Performance Evaluation</a></li></ul></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/References/>References</a></li></ul></li><li><a href=https://chenxingqiang.github.io/llmir-www/pubs/>LLMIR Related Publications</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/users/>Users of MLIR</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/deprecation/>Deprecations & Current Refactoring</a></li><li class=has-sub-menu><a href=https://chenxingqiang.github.io/llmir-www/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Faq/>FAQ</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/openprojects/>Open Projects</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Glossary/>Glossary</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/TestingGuide/>Testing Guide</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/DeveloperGuide/>Developer Guide</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>