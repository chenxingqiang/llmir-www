<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Quantization Support - LLMIR</title>
<meta name=description content="Large Language Model IR Compiler Framework"><meta name=generator content="Hugo 0.129.0"><link href=https://chenxingqiang.github.io/llmir-www/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://chenxingqiang.github.io/llmir-www/docs/architecture/Quantization/><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://chenxingqiang.github.io/llmir-www/js/bundle.js></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://chenxingqiang.github.io/llmir-www//mlir-logo.png width=40px align=absmiddle>
LLMIR</div></h1><span class=version>Version 0.0.1</span><p class=description>Large Language Model IR Compiler Framework</p></header><div class=global-menu><nav><ul><li><a href=/llmir-www/governance/>Governance</a></li><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/llmir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/llmir-www/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/chenxingqiang/llmir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/llmir-www/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/chenxingqiang/llmir>GitHub</a></li></ul></li><li><a href=https://github.com/chenxingqiang/llmir/issues>Bugs</a></li><li><a href=https://github.com/chenxingqiang/llmir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/LLMIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>Quantization Support</h1><h1 id=quantization-support-in-llmir>Quantization Support in LLMIR</h1><p>Quantization is a critical optimization technique for large language models, reducing memory footprint and computation requirements by using lower-precision representations of weights and activations. LLMIR provides comprehensive support for quantization through specialized representations and transformations.</p><h2 id=quantization-in-llmir>Quantization in LLMIR&nbsp;<a class=headline-hash href=#quantization-in-llmir>¶</a></h2><p>LLMIR supports various quantization strategies tailored for LLM inference:</p><h3 id=custom-quantized-types>Custom Quantized Types&nbsp;<a class=headline-hash href=#custom-quantized-types>¶</a></h3><p>LLMIR defines specialized types for representing quantized tensors:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=c>// INT8 asymmetric quantized tensor
</span></span></span><span class=line><span class=cl><span class=c></span><span class=p>!</span>llm<span class=p>.</span>quantized_tensor<span class=p>&lt;</span><span class=m>4x1024x</span><span class=k>i8</span><span class=p>,</span> <span class=nl>scale=</span><span class=k>f32</span><span class=p>,</span> <span class=nl>zp=</span><span class=k>i8</span><span class=p>,</span> <span class=nl>group_size=</span><span class=m>128</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// INT4 symmetric grouped quantized tensor
</span></span></span><span class=line><span class=cl><span class=c></span><span class=p>!</span>llm<span class=p>.</span>quantized_tensor<span class=p>&lt;</span><span class=m>4x1024x</span><span class=k>i4</span><span class=p>,</span> <span class=nl>scale=</span><span class=k>f32</span><span class=p>,</span> <span class=nl>symmetric=</span>true<span class=p>,</span> <span class=nl>group_size=</span><span class=m>128</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// Mixed precision quantized tensor
</span></span></span><span class=line><span class=cl><span class=c></span><span class=p>!</span>llm<span class=p>.</span>mixed_quantized_tensor<span class=p>&lt;</span><span class=m>4x1024x</span><span class=p>!</span>llm<span class=p>.</span>mixed<span class=p>&lt;</span><span class=k>i8</span><span class=p>,</span><span class=k>i4</span><span class=p>&gt;,</span> <span class=nl>scale=</span><span class=k>f32</span><span class=p>&gt;</span>
</span></span></code></pre></div><h3 id=quantization-operations>Quantization Operations&nbsp;<a class=headline-hash href=#quantization-operations>¶</a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-mlir data-lang=mlir><span class=line><span class=cl><span class=c>// Quantize a tensor from FP16 to INT8
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%quantized</span> <span class=p>=</span> llm<span class=p>.</span>quantize<span class=p>(</span><span class=nv>%input</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>scale =</span> dense<span class=p>&lt;</span><span class=m>0.01</span><span class=p>&gt;</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>256x</span><span class=k>f32</span><span class=p>&gt;,</span>
</span></span><span class=line><span class=cl>  <span class=nl>zero_point =</span> dense<span class=p>&lt;</span><span class=m>-2</span><span class=p>&gt;</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>256x</span><span class=k>i8</span><span class=p>&gt;,</span>
</span></span><span class=line><span class=cl>  <span class=nl>bits =</span> <span class=m>8</span> <span class=p>:</span> <span class=k>i32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nl>symmetric =</span> false
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1x256x</span><span class=k>f16</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1x256x</span><span class=k>i8</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// Dequantize from INT8 back to FP16
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%dequantized</span> <span class=p>=</span> llm<span class=p>.</span>dequantize<span class=p>(</span><span class=nv>%quantized</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>scale =</span> dense<span class=p>&lt;</span><span class=m>0.01</span><span class=p>&gt;</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>256x</span><span class=k>f32</span><span class=p>&gt;,</span>
</span></span><span class=line><span class=cl>  <span class=nl>zero_point =</span> dense<span class=p>&lt;</span><span class=m>-2</span><span class=p>&gt;</span> <span class=p>:</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>256x</span><span class=k>i8</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1x256x</span><span class=k>i8</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>1x256x</span><span class=k>f16</span><span class=p>&gt;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c>// Quantized matrix multiplication
</span></span></span><span class=line><span class=cl><span class=c></span><span class=nv>%result</span> <span class=p>=</span> llm<span class=p>.</span>quantized_matmul<span class=p>(</span><span class=nv>%input</span><span class=p>,</span> <span class=nv>%weight</span><span class=p>,</span> <span class=nv>%scales</span><span class=p>,</span> <span class=nv>%zero_points</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nl>bits =</span> <span class=m>8</span> <span class=p>:</span> <span class=k>i32</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nl>group_size =</span> <span class=m>128</span> <span class=p>:</span> <span class=k>i32</span>
</span></span><span class=line><span class=cl><span class=p>}</span> <span class=p>:</span> <span class=p>(</span><span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f16</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>i8</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>f32</span><span class=p>&gt;,</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x</span><span class=k>i8</span><span class=p>&gt;)</span> <span class=p>-&gt;</span> <span class=kt>tensor</span><span class=p>&lt;</span><span class=m>?x?x</span><span class=k>f16</span><span class=p>&gt;</span>
</span></span></code></pre></div><h2 id=quantization-methods>Quantization Methods&nbsp;<a class=headline-hash href=#quantization-methods>¶</a></h2><p>LLMIR will support multiple quantization strategies:</p><h3 id=post-training-quantization-ptq>Post-Training Quantization (PTQ)&nbsp;<a class=headline-hash href=#post-training-quantization-ptq>¶</a></h3><ul><li><strong>Symmetric Quantization</strong>: Uses a symmetric range around zero</li><li><strong>Asymmetric Quantization</strong>: Uses zero-point offsets for asymmetric ranges</li><li><strong>Per-Channel/Per-Tensor</strong>: Supports different scaling granularities</li><li><strong>Group-wise Quantization</strong>: Applies quantization parameters to groups of weights</li></ul><h3 id=quantization-aware-inference-qai>Quantization-Aware Inference (QAI)&nbsp;<a class=headline-hash href=#quantization-aware-inference-qai>¶</a></h3><ul><li><strong>Weight-only Quantization</strong>: Keeps activations in higher precision</li><li><strong>Activation Quantization</strong>: Quantizes intermediate activations</li><li><strong>Mixed-precision Inference</strong>: Different precision for different parts of the model</li></ul><h2 id=optimization-passes>Optimization Passes&nbsp;<a class=headline-hash href=#optimization-passes>¶</a></h2><p>LLMIR will include several quantization-related optimization passes:</p><ol><li><strong>QuantizationCalibrationPass</strong>: Analyze model to determine optimal quantization parameters</li><li><strong>WeightQuantizationPass</strong>: Convert model weights to quantized formats</li><li><strong>ActivationQuantizationPass</strong>: Add quantization/dequantization for activations</li><li><strong>QuantizedOperationFusionPass</strong>: Fuse quantized operations for efficient execution</li><li><strong>HardwareSpecificQuantizationPass</strong>: Customize quantization for specific hardware features</li></ol><h2 id=integration-with-hardware-backends>Integration with Hardware Backends&nbsp;<a class=headline-hash href=#integration-with-hardware-backends>¶</a></h2><p>LLMIR&rsquo;s quantization system is designed to integrate with various hardware backends:</p><ul><li><strong>NVIDIA Tensor Cores</strong>: Support for INT8/INT4 computation</li><li><strong>Intel AMX</strong>: Optimizations for x86 Advanced Matrix Extensions</li><li><strong>ARM Matrix multiplier</strong>: Support for efficient ARM-based inference</li><li><strong>Custom Accelerators</strong>: Extensible for specialized ML hardware</li></ul><h2 id=runtime-support>Runtime Support&nbsp;<a class=headline-hash href=#runtime-support>¶</a></h2><p>The LLMIR runtime will provide efficient implementations for quantized operations:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-cpp data-lang=cpp><span class=line><span class=cl><span class=c1>// Quantized Matrix Multiplication (Planned API)
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=kt>void</span> <span class=nf>quantizedMatMul</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>void</span><span class=o>*</span> <span class=n>input</span><span class=p>,</span>           <span class=o>//</span> <span class=n>Input</span> <span class=n>activations</span> <span class=p>(</span><span class=n>typically</span> <span class=n>FP16</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>const</span> <span class=kt>int8_t</span><span class=o>*</span> <span class=n>weights</span><span class=p>,</span>       <span class=c1>// Quantized weights (INT8/INT4)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=kt>float</span><span class=o>*</span> <span class=n>scales</span><span class=p>,</span>         <span class=c1>// Quantization scales
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>const</span> <span class=kt>int8_t</span><span class=o>*</span> <span class=n>zeroPoints</span><span class=p>,</span>    <span class=c1>// Zero points (for asymmetric)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>void</span><span class=o>*</span> <span class=n>output</span><span class=p>,</span>                <span class=c1>// Output buffer
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>M</span><span class=p>,</span> <span class=kt>int</span> <span class=n>N</span><span class=p>,</span> <span class=kt>int</span> <span class=n>K</span><span class=p>,</span>         <span class=c1>// Matrix dimensions
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>groupSize</span><span class=p>,</span>               <span class=c1>// Group size for scales
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=kt>int</span> <span class=n>bits</span>                     <span class=c1>// Bit width (8/4)
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>);</span>
</span></span></code></pre></div><h2 id=future-directions>Future Directions&nbsp;<a class=headline-hash href=#future-directions>¶</a></h2><p>As part of LLMIR&rsquo;s advanced features (Phase 3), quantization support will be enhanced with:</p><ul><li><strong>Sparse-Quantized Representations</strong>: Combining sparsity and quantization</li><li><strong>Dynamic Quantization</strong>: Adaptive precision based on content</li><li><strong>Calibration Tools</strong>: Utilities for determining optimal quantization parameters</li><li><strong>Automated Mixed Precision</strong>: Intelligent selection of precision for different model parts</li></ul><p>This feature is planned for Phase 3 of the LLMIR project development.</p><div class=edit-meta><br><a href=https://github.com/chenxingqiang/llmir-www//edit/main/docs/content/ class=edit-page><i class="fas fa-pen-square"></i> Edit on GitHub</a></div><nav class=pagination><a class="nav nav-prev" href=https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/ title="KV Cache Optimization"><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - KV Cache Optimization</a>
<a class="nav nav-next" href=https://chenxingqiang.github.io/llmir-www/docs/architecture/DistributedDeployment/ title="Distributed Deployment">Next - Distributed Deployment <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://chenxingqiang.github.io/llmir-www/>Home</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/governance/>Governance</a></li><li class="parent has-sub-menu"><a href=https://chenxingqiang.github.io/llmir-www/docs/>Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class="parent has-sub-menu"><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/>Architecture<span class="mark opened">-</span></a><ul class=sub-menu><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/>KV Cache Optimization</a></li><li class=active><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/Quantization/>Quantization Support</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/DistributedDeployment/>Distributed Deployment</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/PerformanceEvaluation/>Performance Evaluation</a></li></ul></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/References/>References</a></li></ul></li><li><a href=https://chenxingqiang.github.io/llmir-www/pubs/>LLMIR Related Publications</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/users/>Users of MLIR</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/deprecation/>Deprecations & Current Refactoring</a></li><li class=has-sub-menu><a href=https://chenxingqiang.github.io/llmir-www/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Faq/>FAQ</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/openprojects/>Open Projects</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Glossary/>Glossary</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/TestingGuide/>Testing Guide</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/DeveloperGuide/>Developer Guide</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>