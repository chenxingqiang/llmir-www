<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>Architecture - LLMIR</title>
<meta name=description content="Large Language Model IR Compiler Framework"><meta name=generator content="Hugo 0.129.0"><link href=https://chenxingqiang.github.io/llmir-www/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://chenxingqiang.github.io/llmir-www/docs/architecture/><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://chenxingqiang.github.io/llmir-www/js/bundle.js></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://chenxingqiang.github.io/llmir-www//mlir-logo.png width=40px align=absmiddle>
LLMIR</div></h1><span class=version>Version 0.0.1</span><p class=description>Large Language Model IR Compiler Framework</p></header><div class=global-menu><nav><ul><li><a href=/llmir-www/governance/>Governance</a></li><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/llmir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/llmir-www/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/chenxingqiang/llmir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/llmir-www/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/chenxingqiang/llmir>GitHub</a></li></ul></li><li><a href=https://github.com/chenxingqiang/llmir/issues>Bugs</a></li><li><a href=https://github.com/chenxingqiang/llmir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/LLMIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>Architecture</h1><h1 id=llmir-architecture>LLMIR Architecture</h1><p>This section provides detailed information about the LLMIR architecture, its key components, and features.</p><h2 id=key-features>Key Features</h2><p>LLMIR is being developed with several key optimizations for LLM inference:</p><ul><li><a href=/llmir-www/docs/architecture/KVCache/>KV Cache Optimization</a>: Efficient key-value cache management techniques</li><li><a href=/llmir-www/docs/architecture/Quantization/>Quantization Support</a>: Comprehensive quantization capabilities</li><li><a href=/llmir-www/docs/architecture/DistributedDeployment/>Distributed Deployment</a>: Support for multi-device inference</li><li><a href=/llmir-www/docs/architecture/PerformanceEvaluation/>Performance Evaluation</a>: Benchmarking and evaluation methodologies</li></ul><h2 id=system-architecture>System Architecture</h2><p>LLMIR (Large Language Model Intermediate Representation) is a compiler infrastructure for large language models based on MLIR, designed to optimize and accelerate LLM inference through specialized compilation techniques.</p><p>LLMIR follows a layered architecture:</p><pre tabindex=0><code>                       ┌─────────────────┐
                       │   Application   │
                       │ vLLM / SGLang   │
                       └────────┬────────┘
                                │
                                ▼
┌──────────────────────────────────────────────────┐
│                    LLMIR Compiler                │
│                                                  │
│  ┌──────────────┐    ┌───────────────────────┐   │
│  │ Front-end    │ → │  MLIR Optimization     │   │
│  │ Converters   │    │  Pipeline             │   │
│  └──────────────┘    └───────────┬───────────┘   │
│                                  │               │
│                      ┌───────────▼───────────┐   │
│                      │    Backend Generators  │   │
│                      └───────────────────────┘   │
└──────────────────────────┬───────────────────────┘
                           │
                           ▼
            ┌─────────────────────────────┐
            │       Execution Layer       │
            │ CUDA / ROCm / LLVM / Accel  │
            └─────────────────────────────┘
</code></pre><h3 id=front-end-converters>Front-end Converters</h3><p>The front-end converters are responsible for translating models and operations from existing frameworks into the LLMIR representation:</p><ul><li><strong>vLLM Converter</strong>: Translates vLLM&rsquo;s model representation and PagedAttention mechanism into LLMIR</li><li><strong>SGLang Converter</strong>: Maps SGLang&rsquo;s computation graphs to LLMIR operations</li></ul><h3 id=mlir-optimization-pipeline>MLIR Optimization Pipeline</h3><p>The optimization pipeline includes a range of passes specifically designed for LLM inference:</p><ul><li><strong>General Optimizations</strong>: Common compiler optimizations like constant folding, dead code elimination, and loop optimizations</li><li><strong>LLM-Specific Optimizations</strong>: KV cache blocking, attention computation fusion, quantization transformations</li><li><strong>Hardware-Specific Optimizations</strong>: Optimizations targeting specific hardware features</li></ul><h3 id=backend-generators>Backend Generators</h3><p>Backend generators produce optimized code for different execution targets:</p><ul><li><strong>CUDA/HIP Code Generation</strong>: For NVIDIA and AMD GPUs</li><li><strong>LLVM IR Generation</strong>: For CPUs and general platforms</li><li><strong>Specialized Accelerator Code</strong>: For ML accelerators like TPUs</li></ul><h3 id=runtime-library>Runtime Library</h3><p>LLMIR includes a runtime library that provides key functionality:</p><ul><li><strong>Memory Management</strong>: Efficient KV cache allocation and scheduling</li><li><strong>Execution Scheduler</strong>: Dynamic batching and request management</li><li><strong>Device Communication</strong>: Multi-device data exchange for distributed inference</li></ul><h2 id=llmir-dialect>LLMIR Dialect</h2><p>The core of LLMIR is a specialized MLIR dialect for LLM operations, including custom types and operations tailored for LLM workloads. For detailed information about specific features, please visit the dedicated pages listed above.</p><h2 id=development-status>Development Status</h2><p>LLMIR is being developed in phases according to our development plan:</p><ol><li><strong>Phase 1 (Current Focus)</strong>: Building the core infrastructure, including MLIR dialect design and implementation</li><li><strong>Phase 2 (Planned)</strong>: Implementing core optimizations like KV cache management and attention fusion</li><li><strong>Phase 3 (Future)</strong>: Adding advanced features such as quantization, parallelism strategies, and advanced hardware targeting</li></ol><p>For the current status and detailed roadmap, please visit our
<a href=https://github.com/chenxingqiang/llmir.git>GitHub repository</a>.</p><h2 id=references>References</h2><p>For a comprehensive list of related work and publications that have influenced LLMIR, please see our
<a href=/llmir-www/docs/References/>References</a> page.</p><h2>Architecture Docs</h2><ul><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/>KV Cache Optimization</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/Quantization/>Quantization Support</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/DistributedDeployment/>Distributed Deployment</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/PerformanceEvaluation/>Performance Evaluation</a></li></ul><div class=edit-meta><br><a href=https://github.com/chenxingqiang/llmir-www//edit/main/docs/content/ class=edit-page><i class="fas fa-pen-square"></i> Edit on GitHub</a></div><nav class=pagination><a class="nav nav-prev" href=https://chenxingqiang.github.io/llmir-www/docs/ title=Documentation><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - Documentation</a>
<a class="nav nav-next" href=https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/ title="KV Cache Optimization">Next - KV Cache Optimization <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://chenxingqiang.github.io/llmir-www/>Home</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/governance/>Governance</a></li><li class="parent has-sub-menu"><a href=https://chenxingqiang.github.io/llmir-www/docs/>Documentation<span class="mark opened">-</span></a><ul class=sub-menu><li class="active has-sub-menu"><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/>Architecture<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/>KV Cache Optimization</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/Quantization/>Quantization Support</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/DistributedDeployment/>Distributed Deployment</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/PerformanceEvaluation/>Performance Evaluation</a></li></ul></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/References/>References</a></li></ul></li><li><a href=https://chenxingqiang.github.io/llmir-www/pubs/>LLMIR Related Publications</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/users/>Users of MLIR</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/deprecation/>Deprecations & Current Refactoring</a></li><li class=has-sub-menu><a href=https://chenxingqiang.github.io/llmir-www/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Faq/>FAQ</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/openprojects/>Open Projects</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Glossary/>Glossary</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/TestingGuide/>Testing Guide</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/DeveloperGuide/>Developer Guide</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>