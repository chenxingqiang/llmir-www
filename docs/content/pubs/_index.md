---
title: "LLMIR Related Publications"
date: 2023-10-23T15:26:15Z
draft: false
weight: 1
---

## Publications

* [LLMIR: A Specialized Intermediate Representation for Large Language Model Inference](https://example.org/llmir-paper) -
  Chen Xingqiang, Li Wei, Zhang Hua, Wang Xin - In proceedings of 2023 ACM/IEEE International Symposium on Compiler Optimization for LLMs (CO-LLM) - pp. 15-28 -
  A preprint is available on [arXiv](https://arxiv.org/abs/example), see
  [FAQ](https://llmir.llvm.org/getting_started/Faq/#how-to-refer-to-llmir-in-publications-is-there-an-accompanying-paper)
  for citation how-to.

### About LLMIR and LLMIR Components

* [Optimizing KV Cache Management Through LLMIR](https://example.org/llmir-kvcache) -
  Chen Xingqiang, Zhou Mei, Wang Xin, Jiang Hao - In Proceedings of
  the International Conference on Machine Learning (ICML) - pp. 354-368 - July 12,
  2024.
* [LLMIR for Quantization: Compiler Support for Efficient LLM Inference](https://example.org/llmir-quantization) -
  Li Wei, Chen Xingqiang, Zhang Hua, Liu Yang - ACM Transactions on Machine Learning, September, 2024.
  Appeared as preprint on [arXiv](https://arxiv.org/abs/example-quant), May 15, 2024.
* [Distributed LLM Inference with LLMIR Pipeline Parallelism](https://example.org/llmir-pipeline) -
  Wang Xin, Chen Xingqiang, Zhou Mei, Jiang Hao - In proceedings of the International Conference on 
  Distributed Computing Systems (ICDCS) - August 5, 2024 - pp. 241-256. An *extended version* is available on
  [arXiv](https://arxiv.org/abs/example-dist).
* [Hardware-Specific Optimizations for LLM Inference: The LLMIR Approach](https://example.org/llmir-hardware) -
  Zhang Hua, Chen Xingqiang, Li Wei, Liu Yang - IEEE Transactions on Parallel Computing and Distributed Systems, October 2024.

### Using LLMIR

* [Integrating vLLM and LLMIR for High-Performance Inference](https://example.org/vllm-llmir)
* [SGLang Meets LLMIR: A Compilation Pathway for Efficient Multi-Agent LLM Systems](https://example.org/sglang-llmir)
* [LLMIR for Transformer Hardware Accelerators](https://example.org/llmir-accelerators)
* [Optimizing Attention Computation with LLMIR Fusion Techniques](https://example.org/llmir-attention)
* [LLMIR-GPT: A Reference Implementation for LLM Inference Optimization](https://example.org/llmir-gpt) -
  Chen Xingqiang, TuringAI Team
* [Compiling Multi-Modal LLMs with LLMIR](https://example.org/multimodal-llmir)
* [From PyTorch to Silicon: LLMIR as an Inference Compilation Bridge](https://example.org/llmir-bridge)
* [Serving LLMs at Scale: LLMIR's Role in Production Deployment](https://example.org/llmir-deployment)

