<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>FAQ - LLMIR</title>
<meta name=description content="Large Language Model IR Compiler Framework"><meta name=generator content="Hugo 0.129.0"><link href=https://chenxingqiang.github.io/llmir-www/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://chenxingqiang.github.io/llmir-www/getting_started/Faq/><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://chenxingqiang.github.io/llmir-www/js/bundle.js></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://chenxingqiang.github.io/llmir-www//mlir-logo.png width=40px align=absmiddle>
LLMIR</div></h1><span class=version>Version 0.0.1</span><p class=description>Large Language Model IR Compiler Framework</p></header><div class=global-menu><nav><ul><li><a href=/llmir-www/governance/>Governance</a></li><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/llmir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/llmir-www/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/chenxingqiang/llmir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/llmir-www/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/chenxingqiang/llmir>GitHub</a></li></ul></li><li><a href=https://github.com/chenxingqiang/llmir/issues>Bugs</a></li><li><a href=https://github.com/chenxingqiang/llmir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/LLMIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1>FAQ</h1><h1 id=frequently-asked-questions-about-llmir>Frequently Asked Questions about LLMIR</h1><h2 id=what-is-llmir>What is LLMIR?&nbsp;<a class=headline-hash href=#what-is-llmir>¶</a></h2><p>LLMIR (Large Language Model Intermediate Representation) is a compiler infrastructure for large language models based on MLIR (Multi-Level Intermediate Representation). It&rsquo;s designed to optimize and accelerate LLM inference through specialized compilation techniques, providing a unified intermediate representation layer for different LLM frameworks.</p><h2 id=what-problem-does-llmir-solve>What problem does LLMIR solve?&nbsp;<a class=headline-hash href=#what-problem-does-llmir-solve>¶</a></h2><p>LLMIR addresses several key challenges in LLM inference:</p><ol><li><strong>Performance bottlenecks</strong>: By applying compiler optimization techniques, LLMIR reduces inference latency and improves throughput.</li><li><strong>Memory efficiency</strong>: Through optimizations like efficient KV cache management, LLMIR enables processing of longer sequences and larger batch sizes with less memory.</li><li><strong>Framework fragmentation</strong>: LLMIR provides a unified compilation layer that works across different LLM frameworks (like vLLM and SGLang).</li><li><strong>Hardware diversity</strong>: LLMIR enables efficient targeting of different hardware platforms (GPUs, CPUs, accelerators) from the same source representation.</li></ol><h2 id=how-does-llmir-relate-to-mlir>How does LLMIR relate to MLIR?&nbsp;<a class=headline-hash href=#how-does-llmir-relate-to-mlir>¶</a></h2><p>LLMIR is built on top of MLIR, which provides the foundational compiler infrastructure. LLMIR extends MLIR with:</p><ol><li><strong>LLM-specific dialect</strong>: Custom operations and types specifically designed for LLM inference</li><li><strong>Specialized optimizations</strong>: Passes targeting key performance bottlenecks in LLM workloads</li><li><strong>Runtime components</strong>: Libraries for efficient execution of compiled LLM models</li></ol><h2 id=what-is-the-current-status-of-llmir>What is the current status of LLMIR?&nbsp;<a class=headline-hash href=#what-is-the-current-status-of-llmir>¶</a></h2><p>LLMIR is currently in active development. We are following the development plan as outlined in our
<a href=https://github.com/chenxingqiang/llmir.git>GitHub repository</a>. The project is in the early phases of building the core infrastructure and defining the MLIR dialect.</p><h2 id=what-features-does-llmir-support>What features does LLMIR support?&nbsp;<a class=headline-hash href=#what-features-does-llmir-support>¶</a></h2><p>LLMIR is being developed in phases:</p><h3 id=phase-1-current-focus>Phase 1 (Current Focus)&nbsp;<a class=headline-hash href=#phase-1-current-focus>¶</a></h3><ul><li>Basic infrastructure: MLIR dialect, type system, core operations</li></ul><h3 id=phase-2-planned>Phase 2 (Planned)&nbsp;<a class=headline-hash href=#phase-2-planned>¶</a></h3><ul><li>KV cache optimization</li><li>Attention computation fusion</li><li>Memory management optimizations</li></ul><h3 id=phase-3-future>Phase 3 (Future)&nbsp;<a class=headline-hash href=#phase-3-future>¶</a></h3><ul><li>Quantization support</li><li>Parallelism strategies (tensor/pipeline)</li><li>Advanced backend code generation</li></ul><h2 id=how-can-i-contribute-to-llmir>How can I contribute to LLMIR?&nbsp;<a class=headline-hash href=#how-can-i-contribute-to-llmir>¶</a></h2><p>Contributions to LLMIR are welcome! Here&rsquo;s how you can contribute:</p><ol><li>Clone the
<a href=https://github.com/chenxingqiang/llmir.git>repository</a> and familiarize yourself with the code</li><li>Check the development plan to see which areas need attention</li><li>Follow standard MLIR development practices</li><li>Submit pull requests with well-tested changes</li></ol><h2 id=which-llm-frameworks-does-llmir-support>Which LLM frameworks does LLMIR support?&nbsp;<a class=headline-hash href=#which-llm-frameworks-does-llmir-support>¶</a></h2><p>LLMIR is being designed to initially support:</p><ol><li><strong>vLLM</strong>: For its efficient paged attention mechanism</li><li><strong>SGLang</strong>: For its structured generation capabilities</li></ol><p>Additional framework support may be added in the future.</p><h2 id=what-hardware-targets-does-llmir-support>What hardware targets does LLMIR support?&nbsp;<a class=headline-hash href=#what-hardware-targets-does-llmir-support>¶</a></h2><p>LLMIR aims to support:</p><ol><li><strong>NVIDIA GPUs</strong> (via CUDA code generation)</li><li><strong>AMD GPUs</strong> (via ROCm/HIP)</li><li><strong>x86/ARM CPUs</strong> (via LLVM)</li><li><strong>Specialized accelerators</strong> (future support)</li></ol><h2 id=how-does-llmir-compare-to-other-llm-optimization-frameworks>How does LLMIR compare to other LLM optimization frameworks?&nbsp;<a class=headline-hash href=#how-does-llmir-compare-to-other-llm-optimization-frameworks>¶</a></h2><p>Unlike framework-specific optimizers, LLMIR provides a cross-framework compilation layer. Compared to:</p><ul><li><strong>TensorRT-LLM</strong>: LLMIR offers more flexibility in model representation and isn&rsquo;t tied to a specific vendor</li><li><strong>Framework-native optimizers</strong>: LLMIR enables cross-framework optimizations and consistent hardware targeting</li><li><strong>Generic ML compilers</strong>: LLMIR includes specialized optimizations for LLM inference patterns</li></ul><h2 id=how-can-i-use-llmir-in-my-project>How can I use LLMIR in my project?&nbsp;<a class=headline-hash href=#how-can-i-use-llmir-in-my-project>¶</a></h2><p>LLMIR is still in the early development phase. When ready for use, it will provide:</p><ol><li>A C++ API for integrating into compilation workflows</li><li>Python bindings for easy integration with Python-based ML frameworks</li><li>Command-line tools for converting and optimizing models</li></ol><h2 id=where-can-i-learn-more-about-llmir>Where can I learn more about LLMIR?&nbsp;<a class=headline-hash href=#where-can-i-learn-more-about-llmir>¶</a></h2><p>The primary resources for learning about LLMIR are:</p><ol><li><a href=https://github.com/chenxingqiang/llmir.git>GitHub Repository</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/>Project Website</a></li><li><a href=/llmir-www/getting_started/DeveloperGuide/>Developer Guide</a></li></ol><h2 id=how-to-refer-to-mlir-in-publications-is-there-an-accompanying-paper>How to refer to MLIR in publications? Is there an accompanying paper?&nbsp;<a class=headline-hash href=#how-to-refer-to-mlir-in-publications-is-there-an-accompanying-paper>¶</a></h2><p>MLIR has been presented in the 2021 IEEE/ACM International Symposium on Code
Generation and Optimization, the full text of the paper is
<a href=https://ieeexplore.ieee.org/abstract/document/9370308>available from
IEEE</a>. A pre-publication
draft is available on
<a href=https://arxiv.org/pdf/2002.11054>arXiv</a> but may be
missing improvements and corrections. Please also note that MLIR keeps evolving
and IR snippets presented in the paper may no longer use modern syntax, refer to
the MLIR documentation for the new syntax.</p><p>To cite MLIR in academic or other publications, please use: <em>Chris Lattner,
Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River
Riddle, Tatiana Shpeisman, Nicolas Vasilache, and Oleksandr Zinenko. &ldquo;MLIR:
Scaling compiler infrastructure for domain specific computation.&rdquo; In 2021
IEEE/ACM International Symposium on Code Generation and Optimization (CGO), pp.
2-14. IEEE, 2021.</em></p><p>The BibTeX entry is as follows.</p><pre tabindex=0><code>@inproceedings{mlir,
  author={Lattner, Chris and Amini, Mehdi and Bondhugula, Uday and Cohen, Albert and Davis, Andy and Pienaar, Jacques and Riddle, River and Shpeisman, Tatiana and Vasilache, Nicolas and Zinenko, Oleksandr},
  booktitle={2021 {{IEEE/ACM}} International Symposium on Code Generation and Optimization (CGO)},
  title={{{MLIR}}: Scaling Compiler Infrastructure for Domain Specific Computation},
  year={2021},
  volume={},
  number={},
  pages={2-14},
  doi={10.1109/CGO51591.2021.9370308}
}
</code></pre><p>Please do <strong>not</strong> cite the arXiv preprint as it is not a formal peer-reviewed
publication.</p><h2 id=why-is-small-feature-not-available-in-mlir>Why is &lt;small feature> not available in MLIR?&nbsp;<a class=headline-hash href=#why-is-small-feature-not-available-in-mlir>¶</a></h2><p>On general basis, there is never a reason why a small feature is not available in MLIR other than nobody needed it enough to implement it. Consider submitting a patch. For larger features and dialects, follow the
<a href=https://mlir.llvm.org/getting_started/DeveloperGuide/#guidelines-on-contributing-a-new-dialect-or-important-components>request-for-comments</a> process.</p><h2 id=mlir-is-too-heavy-framework-should-i-just-reimplement-my-own-compiler-from-scratch>MLIR is too heavy framework, should I just reimplement my own compiler from scratch?&nbsp;<a class=headline-hash href=#mlir-is-too-heavy-framework-should-i-just-reimplement-my-own-compiler-from-scratch>¶</a></h2><p>Maybe: it is hard to tell as it depends on your requirements, even C++ may already be too
large for some micro-controllers. In our experience most projects ends up growing beyond
what their original author intended, and reimplementing the features you would get from
MLIR would also have a footprint. MLIR footprint is representative of the features it
provides. More importantly we have a &ldquo;you don&rsquo;t pay for what you don&rsquo;t use&rdquo; approach:
MLIR is very modular and you can link a binary with a very minimal set of libraries.
If you use just the core IR, some pieces of the infrastructure, and a few dialects
you should expect a few MBs. We have
<a href=https://github.com/llvm/llvm-project/tree/main/mlir/examples/minimal-opt>three examples</a>
in the repo showing some small possible configurations of MLIR, showing that the
core of MLIR can take around 1MB.</p><h2 id=what-is-the-difference-between-the-tensor-and-vector-types>What is the difference between the Tensor and Vector types?&nbsp;<a class=headline-hash href=#what-is-the-difference-between-the-tensor-and-vector-types>¶</a></h2><ol><li>Conceptual: vectors are meant to and occur in lower level dialects - often where you expect hardware to have registers of that size. Tensors model higher-level &ldquo;closer to the source&rdquo; abstract representation. This is reflected in the abstraction modeled by the operations from the
<a href=https://mlir.llvm.org/docs/Dialects/Vector/><code>vector</code> dialect</a>, while Tensors would be more naturally present in the operations of the
<a href=https://mlir.llvm.org/docs/Dialects/Linalg/><code>linalg</code> dialect</a>.</li><li>Tensors can be dynamically shaped, unranked, or have 0 dimensions ; but Vectors can&rsquo;t be.</li><li>You can have a memref (a buffer in memory) containing Vectors but you can&rsquo;t have a memref of a tensor type.</li><li>The set of allowed element types is different: the Tensor type isn&rsquo;t limited while Vector is limited to float and integer types.</li><li>Tensors accept an optional &ldquo;encoding&rdquo; attribute, vector don&rsquo;t at the moment.</li></ol><h2 id=registered-loaded-dependent-whats-up-with-dialects-management>Registered, loaded, dependent: what&rsquo;s up with Dialects management?&nbsp;<a class=headline-hash href=#registered-loaded-dependent-whats-up-with-dialects-management>¶</a></h2><p>Before creating an Operation, a Type, or an Attribute, the associated Dialect
must be already <em>loaded</em> in the <code>MLIRContext</code>. For example the Toy tutorial
explicitly loads the Toy Dialect before emitting the Toy IR from the AST.</p><p>The process of loading a Dialect in the context is not thread-safe, which forces
all involved Dialects to be loaded before the multi-threaded pass manager starts
the execution. To keep the system modular and layered, invoking a pass pipeline
should never require pre-loading dialects explicitly. This is achieved by
requiring every pass to declare a list of <em>dependent</em> Dialects: these are
Dialects for which an entity (Operation, Type, or Attribute) can be created by
the pass, other than for Dialects that would already be in the input.
For example, a <code>convertLinalgToLoops</code> pass would declare the <code>SCF</code> Dialect as
dependent, but does not need to declare <code>Linalg</code>. See also
<a href=https://mlir.llvm.org/docs/PassManagement/#dependent-dialects>dependent dialects</a>
in the pass infrastructure documentation.</p><p>Finally, dialects can be <em>registered</em> with the context. The sole purpose of the
registration is to make these dialects available for the textual parser used by
tools like <code>mlir-opt</code> or <code>mlir-translate</code>. A compiler frontend emitting the IR
programmatically and invoking a pass pipeline would never need to register any
dialects.</p><h2 id=in-dialect-conversion-i-want-an-operation-to-be-removed-after-its-users-get-converted-how-do-i-do-that>In dialect conversion, I want an operation to be removed after its users get converted, how do I do that?&nbsp;<a class=headline-hash href=#in-dialect-conversion-i-want-an-operation-to-be-removed-after-its-users-get-converted-how-do-i-do-that>¶</a></h2><p>This operation can be marked &ldquo;illegal&rdquo; and you can just do speculatively
<code>rewriter.eraseOp(op);</code>. The operation won&rsquo;t be actually removed right now,
instead when mark something as erased you are basically saying to the driver
&ldquo;I expect all uses of this to go away by the time everything is over&rdquo;. The
conversion will fail if the operation you marked as erased doesn&rsquo;t actually get
erased at the end.</p><h2 id=why-is-dialect-x-missing-feature-y>Why is dialect X missing feature Y?&nbsp;<a class=headline-hash href=#why-is-dialect-x-missing-feature-y>¶</a></h2><p>Most likely, nobody has had a need for it yet. Many MLIR components, dialects
even more than others, grew out of specific needs and are extended by volunteers
sending patches to add the missing bits. Everybody is welcome to contribute!</p><p>In some specfic cases, the dialect design might have explicitly decided against
implementing a feature or chose an alternative modeling that provides a similar
functionality. Such design decisions are usually noted in the dialect or
rationale documents.</p><h2 id=many-dialects-define-a-constant-operation-how-do-i-get-a-constant-value-generically>Many dialects define a <code>constant</code> operation, how do I get a constant value generically?&nbsp;<a class=headline-hash href=#many-dialects-define-a-constant-operation-how-do-i-get-a-constant-value-generically>¶</a></h2><div class=highlight><pre tabindex=0 class=chroma><code class=language-c++ data-lang=c++><span class=line><span class=cl><span class=cp>#include</span> <span class=cpf>&#34;mlir/IR/Matchers.h&#34;</span><span class=cp>
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=c1>// Return the constant attribute, or null if the Operation isn&#39;t a constant.
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>Attribute</span> <span class=nf>getConstantAttr</span><span class=p>(</span><span class=n>Operation</span> <span class=o>*</span><span class=n>constantOp</span><span class=p>)</span> <span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=n>Attribute</span> <span class=n>constant</span><span class=p>;</span>
</span></span><span class=line><span class=cl>  <span class=n>matchPattern</span><span class=p>(</span><span class=n>value</span><span class=p>.</span><span class=n>getDefiningOp</span><span class=p>(),</span> <span class=n>m_Constant</span><span class=p>());</span>
</span></span><span class=line><span class=cl>  <span class=k>return</span> <span class=n>constant</span><span class=p>;</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><h2 id=what-is-the-difference-between-traits-and-interfaces>What is the difference between traits and interfaces?&nbsp;<a class=headline-hash href=#what-is-the-difference-between-traits-and-interfaces>¶</a></h2><p>Both
<a href=https://mlir.llvm.org/docs/Traits/>traits</a> and
<a href=https://mlir.llvm.org/docs/Interfaces>interfaces</a> can be used to inject common
behavior into operations, types and attributes without introducing duplication.
However, conceptually these are quite different.</p><p>Traits inject static behavior into operations/types/attributes whereas
interfaces dynamically dispatch behavior based on their runtime type. For
instance, since
<a href=https://github.com/llvm/llvm-project/blob/f3e1f44340dc26e3810d601edf0e052813b7a11c/mlir/include/mlir/IR/BuiltinOps.td#L167><code>ModuleOp</code></a>
implements the
<a href=https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/IR/SymbolTable.h#L338><code>SymbolTable</code></a>
trait, <code>mlir::ModuleOp</code> exposes <code>lookupSymbol</code> as a member function. However,
there is no type-erased way to access this functionality &ndash; it is available only
via <code>mlir::ModuleOp</code>. On the other hand, if an operation implements
<a href=https://github.com/llvm/llvm-project/blob/902184e6cc263e4c66440c95a21665b6fdffe57c/mlir/include/mlir/Interfaces/CallInterfaces.td#L25><code>CallOpInterface</code></a>,
its implementation of <code>getCallableForCallee</code> can be invoked in a type-erased
manner by <code>dyn_cast</code>ing the operation to a <code>CallOpInterface</code>. The caller does
not need to know the concrete type of the operation to do this.</p><p>There is one similarity between interfaces and traits: both their presence can
be checked dynamically (i.e. without access to the concrete type).
Specifically, presence of traits can be checked using
<a href=https://github.com/llvm/llvm-project/blob/902184e6cc263e4c66440c95a21665b6fdffe57c/mlir/include/mlir/IR/Operation.h#L470><code>Operation::hasTrait</code></a>
and presence of interfaces can be checked using <code>isa&lt;></code>. However, this
similarity does not run deep, and was only added for practical ergonomic
reasons.</p><h2 id=how-to-convert-a-memref-to-a-pointer>How to convert a <code>memref</code> to a pointer?&nbsp;<a class=headline-hash href=#how-to-convert-a-memref-to-a-pointer>¶</a></h2><p>It is impossible in the general case. Structured memory reference (<code>memref</code>) type <strong>is not (only) a pointer</strong>. This type supports multi-dimensional indexing and customizable data layouts to support advanced yet analyzable addressing modes. Implementing address computation requires understanding the layout and storing additional information such as sizes and layout parameters that would be impossible with a plain, single-typed pointer to a contiguous block of data. Even the single-dimensional <code>memref&lt;?xi8></code> with the default layout is <em>not a pointer</em> as it must store at the very least the size of the data (think C++ <code>std::string</code> vs. C <code>NULL</code>-terminated <code>const char *</code>).</p><p>It is, however, possible to define operations that create pointer-like types out of a <code>memref</code> as well as operations that, conversely, create <code>memref</code> out of pointers combined with additional information. Before implementing such operations, dialect authors are advised to carefully consider the implication of such operations on aliasing properties of the resulting IR.</p><p>Interoperability with C is often cited to motivate an opaque cast from <code>memref</code>s to pointers. The
<a href=https://mlir.llvm.org/docs/TargetLLVMIR/#ranked-memref-types>LLVM IR target</a> provides an interface compatible with C for a well-defined subset of <code>memrefs</code> with
<a href=https://mlir.llvm.org/docs/Dialects/Builtin/#strided-memref>strided layout</a>. At the function boundary, it even provides a minimalist support for passing memrefs as
<a href=https://mlir.llvm.org/docs/TargetLLVMIR/#bare-pointer-calling-convention-for-ranked-memref>bare pointers</a> provided their sizes are known statically and their layout is trivially identity.</p><h2 id=whats-with-op-symbol-declaration-cannot-have-public-visibility>What&rsquo;s with &ldquo;op symbol declaration cannot have public visibility&rdquo;?&nbsp;<a class=headline-hash href=#whats-with-op-symbol-declaration-cannot-have-public-visibility>¶</a></h2><p>A common mistake is to try to provide a function declaration (that is a function
without a body) but leaving it &ldquo;public&rdquo;. Declaration must be private, only
definitions can be public in the MLIR symbol system. See the
<a href=https://mlir.llvm.org/docs/SymbolsAndSymbolTables/#symbol-visibility>symbol visibility</a>
documentation.</p><h2 id=im-confused-about-iterating-on-getusers-vs-getuses-whats-the-difference>I&rsquo;m confused about iterating on <code>getUsers()</code> vs <code>getUses()</code>: what&rsquo;s the difference?&nbsp;<a class=headline-hash href=#im-confused-about-iterating-on-getusers-vs-getuses-whats-the-difference>¶</a></h2><p>The &ldquo;users&rdquo; of an SSA value are instances of <code>Operation</code>, while the &ldquo;uses&rdquo; refer to
the operands of these operations. For example considering <code>test.op(%0, %0) : ...</code>, when
iterating on the &ldquo;uses&rdquo; of <code>%0</code> you would see two instances of <code>OpOperand</code> (one for each
use in <code>test.op</code>), whereas iterating on the &ldquo;users&rdquo; of <code>%0</code> would yield directly two
<code>Operation *</code> corresponding to <code>test.op</code>. Note that you see <code>test.op</code> twice as it is
twice a user of <code>%0</code>, it&rsquo;s up to the call site to use a set to unique these if needed.
<a href=https://mlir.llvm.org/docs/Tutorials/UnderstandingTheIRStructure/#traversing-the-def-use-chains>The tutorial on use-def chains</a> may help understand the details as well.</p><h2 id=how-to-programmatically-obtain-the-name-of-the-ssa-value-foo>How to programmatically obtain the &ldquo;name&rdquo; of the SSA value (<code>%foo</code>)?&nbsp;<a class=headline-hash href=#how-to-programmatically-obtain-the-name-of-the-ssa-value-foo>¶</a></h2><p>The values names are <em>not part of the IR</em> and are only there to make textual
representation of the IR easier for humans to read. They are generated by the
IR printer on-the-fly and may differ depending on the printer configuration.
While it is technically possible to configure the printer to produce predictable
names, in particular names with specific prefixes via the
<a href=https://github.com/llvm/llvm-project/blob/main/mlir/include/mlir/IR/OpAsmInterface.td><code>OpAsmOpInterface</code></a>,
one is strongly discouraged from relying on the textual names. Therefore there
is intentionally no support for obtaining these names easily.</p><div class=edit-meta><br><a href=https://github.com/chenxingqiang/llmir-www//edit/main/docs/content/ class=edit-page><i class="fas fa-pen-square"></i> Edit on GitHub</a></div><nav class=pagination><a class="nav nav-prev" href=https://chenxingqiang.github.io/llmir-www/getting_started/ title="Getting Started"><i class="fas fa-arrow-left" aria-hidden=true></i> Prev - Getting Started</a>
<a class="nav nav-next" href=https://chenxingqiang.github.io/llmir-www/getting_started/ReportingIssues/ title="Reporting Issues">Next - Reporting Issues <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li><a href=https://chenxingqiang.github.io/llmir-www/>Home</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/governance/>Governance</a></li><li class=has-sub-menu><a href=https://chenxingqiang.github.io/llmir-www/docs/>Documentation<span class="mark closed">+</span></a><ul class=sub-menu><li class=has-sub-menu><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/>Architecture<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/>KV Cache Optimization</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/Quantization/>Quantization Support</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/DistributedDeployment/>Distributed Deployment</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/PerformanceEvaluation/>Performance Evaluation</a></li></ul></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/References/>References</a></li></ul></li><li><a href=https://chenxingqiang.github.io/llmir-www/pubs/>LLMIR Related Publications</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/users/>Users of MLIR</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/deprecation/>Deprecations & Current Refactoring</a></li><li class="parent has-sub-menu"><a href=https://chenxingqiang.github.io/llmir-www/getting_started/>Getting Started<span class="mark opened">-</span></a><ul class=sub-menu><li class=active><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Faq/>FAQ</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/openprojects/>Open Projects</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Glossary/>Glossary</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/TestingGuide/>Testing Guide</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/DeveloperGuide/>Developer Guide</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>