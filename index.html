<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><title>LLMIR</title>
<meta name=description content="Large Language Model IR Compiler Framework"><meta name=generator content="Hugo 0.129.0"><link href=https://chenxingqiang.github.io/llmir-www/index.xml rel=alternate type=application/rss+xml><link rel=canonical href=https://chenxingqiang.github.io/llmir-www/><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/theme.css><script src=https://use.fontawesome.com/releases/v5.0.6/js/all.js></script><link rel=stylesheet href=https://chenxingqiang.github.io/llmir-www/css/chroma.min.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js></script><script src=https://cdn.jsdelivr.net/npm/jquery.easing@1.4.1/jquery.easing.min.js></script><script src=https://chenxingqiang.github.io/llmir-www/js/bundle.js></script><script type=text/javascript src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script type=text/x-mathjax-config>
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$', '$'] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
    }
  });
</script><link rel=apple-touch-icon sizes=180x180 href="/apple-touch-icon.png?v=1"><link rel=icon type=image/png sizes=32x32 href="/favicon-32x32.png?v=1"><link rel=icon type=image/png sizes=16x16 href="/favicon-16x16.png?v=1"><link rel=manifest href="/site.webmanifest?v=1"><link rel=mask-icon href="/safari-pinned-tab.svg?v=1" color=#3775e0><link rel="shortcut icon" href="/favicon.ico?v=1"><meta name=msapplication-TileColor content="#2d89ef"><meta name=theme-color content="#ffffff"><link rel=icon href=/favicon.svg type=image/svg+xml sizes=any><style>:root{}</style></head><body><div class=container><header><h1><div><img src=https://chenxingqiang.github.io/llmir-www//mlir-logo.png width=40px align=absmiddle>
LLMIR</div></h1><span class=version>Version 0.0.1</span><p class=description>Large Language Model IR Compiler Framework</p></header><div class=global-menu><nav><ul><li><a href=/llmir-www/governance/>Governance</a></li><li class=parent><a href>Community<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=https://llvm.discourse.group/c/llmir/31>Forums</a></li><li class=child><a href=https://discord.gg/xS7Z362>Chat</a></li></ul></li><li><a href=/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=/llmir-www/getting_started/Faq/>FAQ</a></li><li class=parent><a href=https://github.com/chenxingqiang/llmir>Source<i class="fas fa-angle-right"></i></a><ul class=sub-menu><li class=child><a href=/llmir-www/doxygen/>Doxygen</a></li><li class=child><a href=https://github.com/chenxingqiang/llmir>GitHub</a></li></ul></li><li><a href=https://github.com/chenxingqiang/llmir/issues>Bugs</a></li><li><a href=https://github.com/chenxingqiang/llmir-www/tree/main/website/static/LogoAssets>Logo Assets</a></li><li><a href=https://www.youtube.com/LLMIRCompiler>Youtube Channel</a></li></ul></nav></div><div class=content-container><main><h1 id=large-language-model-intermediate-representation-overview>Large Language Model Intermediate Representation Overview</h1><p>The LLMIR project is a novel approach to building reusable and extensible
compiler infrastructure for large language model inference. LLMIR aims to unify and optimize
LLM inference workflows, improve compilation for heterogeneous hardware, significantly reduce
inference latency, and enhance integration between various LLM frameworks.</p><p>LLMIR is a dedicated compilation middle layer for platform architects and developers, built on
the MLIR framework. It leverages MLIR&rsquo;s flexible infrastructure to represent and transform
computational graphs. LLMIR can integrate with multiple LLM inference frameworks (like vLLM, SGLang)
by converting their high-level operators or model graphs into a unified intermediate representation
for further optimization.</p><h2 id=project-status>Project Status</h2><p>LLMIR is currently in active development. The project is following the development plan as outlined in our
<a href=https://github.com/chenxingqiang/llmir.git>GitHub repository</a>. We are in the early phases of building the core infrastructure and MLIR dialect definition.</p><h2 id=project-resources>Project Resources</h2><p>For more information on LLMIR, please see:</p><ul><li><a href=https://github.com/chenxingqiang/llmir.git>Project Repository</a></li><li><a href=/llmir-www/getting_started/DeveloperGuide/>Developer Guide</a> for getting started with LLMIR.</li></ul><h2 id=what-is-llmir-for>What is LLMIR for?</h2><p>LLMIR is an intermediate representation specialized for optimizing large language model inference. It provides:</p><ul><li>The ability to represent inference workflows from popular LLM frameworks (such as vLLM, SGLang), including
dynamic shapes, batching strategies, and framework-specific operators.</li><li>Optimizations and transformations specifically designed for LLM inference (e.g. attention fusion, KV cache management).</li><li>Cross-framework end-to-end compilation for LLM inference, enabling optimizations like attention computation fusion,
KV cache management, quantization, and pipeline parallelism.</li><li>Ability to target various hardware platforms (GPU, TPU, ASIC, CPU) efficiently by leveraging the MLIR ecosystem.</li><li>Representation of hardware-specific operations for accelerators specialized in LLM workloads.</li></ul><h2 id=core-value-proposition>Core Value Proposition</h2><p>Compared to using the native execution paths of individual frameworks, LLMIR&rsquo;s core value lies in providing cross-framework
compilation capabilities for end-to-end optimization. This includes:</p><ul><li><strong>Performance Improvement</strong>: Leveraging compilation optimizations to reduce inference latency and increase throughput</li><li><strong>Resource Efficiency</strong>: Optimizing memory usage, supporting longer sequences and larger batch sizes</li><li><strong>Scalability</strong>: Supporting different hardware platforms and inference frameworks</li><li><strong>Usability</strong>: Providing developer-friendly APIs to lower integration barriers</li></ul><h2 id=key-features-under-development>Key Features (Under Development)</h2><ul><li><strong>PagedKVCache</strong>: Efficient key-value cache implementation for optimized attention computation</li><li><strong>MLIR Dialect for LLMs</strong>: Custom operations and types for language model inference</li><li><strong>Memory Optimizations</strong>: Block-based memory management for efficient, low-fragmentation memory usage</li><li><strong>Multi-sequence Support</strong>: Handle multiple concurrent sequences with varying lengths</li><li><strong>Hardware Targeting</strong>: Backend code generation for various platforms</li></ul><h1 id=weekly-public-meeting>Weekly Public Meeting</h1><p>We host a <strong>weekly public meeting</strong> about LLMIR and the ecosystem.
To be notified of the next meeting, please subscribe to the
<a href=https://discourse.llvm.org/c/llmir/llmir-announcements/44>LLMIR Announcements</a>
category on Discourse.</p><p>You can register to
<a href="https://calendar.google.com/calendar/u/0?cid=N2EzMDU3NTBjMjkzYWU5MTY5NGNlMmQ3YjJlN2JjNWEyYjViNjg1NTRmODcxOWZiOTU1MmIzNGQxYjkwNGJkZEBncm91cC5jYWxlbmRhci5nb29nbGUuY29t">this public calendar</a>
to keep up-to-date with the schedule.</p><p>If you&rsquo;d like to discuss a particular topic or have questions, please add it to the
<a href=https://docs.google.com/document/d/1y2YlcOVMPocQjSFi3X6gYGRjA0onyqr41ilXji10phw/edit#>agenda doc</a>.</p><h2 id=more-resources>More resources</h2><p>For more information on LLMIR, please see:</p><ul><li>The LLMIR section of the
<a href=https://llvm.discourse.group/c/llmir/31>LLVM forums</a> for any questions.</li><li>Real-time discussion on the LLMIR channel of the
<a href=https://discord.gg/xS7Z362>LLVM discord</a> server.</li></ul><h2 id=citing-llmir>Citing LLMIR</h2><p>Please see the
<a href=https://llmir.llvm.org/getting_started/Faq/#how-to-refer-to-llmir-in-publications-is-there-an-accompanying-paper>FAQ
entry</a>
on how to cite LLMIR in publications.</p><nav class=pagination><a class="nav nav-next" href=https://chenxingqiang.github.io/llmir-www/governance/ title=Governance>Next - Governance <i class="fas fa-arrow-right" aria-hidden=true></i></a></nav><div class=edit-meta><br><a href=https://github.com/chenxingqiang/llmir-www//edit/main/docs/content/ class=edit-page><i class="fas fa-pen-square"></i> Edit on GitHub</a></div><footer><p class=powered>Powered by <a href=https://gohugo.io>Hugo</a>. Theme by <a href=https://themes.gohugo.io/hugo-theme-techdoc/>TechDoc</a>. Designed by <a href=https://github.com/thingsym/hugo-theme-techdoc>Thingsym</a>.</p></footer></main><div class=sidebar><nav class=slide-menu><ul><li class=active><a href=https://chenxingqiang.github.io/llmir-www/>Home</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/governance/>Governance</a></li><li class=has-sub-menu><a href=https://chenxingqiang.github.io/llmir-www/docs/>Documentation<span class="mark closed">+</span></a><ul class=sub-menu><li class=has-sub-menu><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/>Architecture<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/>KV Cache Optimization</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/Quantization/>Quantization Support</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/DistributedDeployment/>Distributed Deployment</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/architecture/PerformanceEvaluation/>Performance Evaluation</a></li></ul></li><li><a href=https://chenxingqiang.github.io/llmir-www/docs/References/>References</a></li></ul></li><li><a href=https://chenxingqiang.github.io/llmir-www/pubs/>LLMIR Related Publications</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/users/>Users of MLIR</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/deprecation/>Deprecations & Current Refactoring</a></li><li class=has-sub-menu><a href=https://chenxingqiang.github.io/llmir-www/getting_started/>Getting Started<span class="mark closed">+</span></a><ul class=sub-menu><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Faq/>FAQ</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/ReportingIssues/>Reporting Issues</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Debugging/>Debugging Tips</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Contributing/>How to Contribute</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/openprojects/>Open Projects</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/Glossary/>Glossary</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/TestingGuide/>Testing Guide</a></li><li><a href=https://chenxingqiang.github.io/llmir-www/getting_started/DeveloperGuide/>Developer Guide</a></li></ul></li></ul></nav><div class=sidebar-footer></div></div></div><a href=# id=backtothetop-fixed class=backtothetop data-backtothetop-duration=600 data-backtothetop-easing=easeOutQuart data-backtothetop-fixed-fadein=1000 data-backtothetop-fixed-fadeout=1000 data-backtothetop-fixed-bottom=10 data-backtothetop-fixed-right=20><span class="fa-layers fa-fw"><i class="fas fa-circle"></i>
<i class="fas fa-arrow-circle-up"></i></span></a></div></body></html>