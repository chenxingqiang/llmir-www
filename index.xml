<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>LLMIR</title><link>https://chenxingqiang.github.io/llmir-www/</link><description>Recent content on LLMIR</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Thu, 09 May 2024 15:26:15 +0000</lastBuildDate><atom:link href="https://chenxingqiang.github.io/llmir-www/index.xml" rel="self" type="application/rss+xml"/><item><title>KV Cache Optimization</title><link>https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/</link><pubDate>Thu, 09 May 2024 15:26:15 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/docs/architecture/KVCache/</guid><description>KV Cache Optimization in LLMIR KV cache management is one of the core optimizations in LLMIR, focusing on efficient handling of key-value pairs in attention mechanisms for transformer-based LLMs.
The KV Cache Challenge In large language model inference, the key-value cache stores computed key and value tensors from previous tokens to avoid redundant computation. As sequence lengths grow, efficiently managing this cache becomes critical for:
Memory efficiency Computation speed Support for longer contexts Dynamic batch handling PagedAttention in LLMIR LLMIR implements a paged attention mechanism inspired by vLLM&amp;rsquo;s approach, which treats the KV cache as blocks of memory rather than a continuous buffer:</description></item><item><title>Quantization Support</title><link>https://chenxingqiang.github.io/llmir-www/docs/architecture/Quantization/</link><pubDate>Thu, 09 May 2024 15:26:15 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/docs/architecture/Quantization/</guid><description>Quantization Support in LLMIR Quantization is a critical optimization technique for large language models, reducing memory footprint and computation requirements by using lower-precision representations of weights and activations. LLMIR provides comprehensive support for quantization through specialized representations and transformations.
Quantization in LLMIR LLMIR supports various quantization strategies tailored for LLM inference:
Custom Quantized Types LLMIR defines specialized types for representing quantized tensors:
// INT8 asymmetric quantized tensor !llm.quantized_tensor&amp;lt;4x1024xi8, scale=f32, zp=i8, group_size=128&amp;gt; // INT4 symmetric grouped quantized tensor !</description></item><item><title>Distributed Deployment</title><link>https://chenxingqiang.github.io/llmir-www/docs/architecture/DistributedDeployment/</link><pubDate>Thu, 09 May 2024 15:26:15 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/docs/architecture/DistributedDeployment/</guid><description>Distributed Deployment in LLMIR LLMIR provides comprehensive support for distributed LLM inference, enabling efficient execution of large models across multiple devices and nodes. This capability is essential for deploying models that exceed the memory capacity of a single device or require higher throughput.
Parallelism Strategies LLMIR supports multiple parallelism strategies for distributed inference:
Tensor Parallelism Tensor parallelism splits individual tensors across multiple devices, allowing for parallel computation of large matrix operations:</description></item><item><title>Performance Evaluation</title><link>https://chenxingqiang.github.io/llmir-www/docs/architecture/PerformanceEvaluation/</link><pubDate>Thu, 09 May 2024 15:26:15 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/docs/architecture/PerformanceEvaluation/</guid><description>Performance Evaluation in LLMIR LLMIR includes comprehensive benchmarking and evaluation methodologies to measure its impact on LLM inference performance across different models and hardware platforms.
Benchmark Framework LLMIR will provide a dedicated benchmarking framework to evaluate performance improvements:
// LLMIR Benchmark API (Planned) class LLMIRBenchmark { public: // Configure benchmark parameters void setModel(const std::string&amp;amp; modelPath); void setHardware(const std::string&amp;amp; hardware); void setSequenceLength(int length); void setBatchSize(int batchSize); void setQuantizationMode(QuantMode quantMode); void setKVCacheStrategy(KVCacheMode kvMode); // Run benchmarks BenchmarkResult runThroughputTest(int iterations); BenchmarkResult runLatencyTest(int iterations); BenchmarkResult runMemoryTest(); // Compare with baselines ComparisonResult compareWithBaseline(const std::string&amp;amp; baselineFramework); }; Key Performance Metrics LLMIR will track and optimize for several key performance metrics:</description></item><item><title>References</title><link>https://chenxingqiang.github.io/llmir-www/docs/References/</link><pubDate>Thu, 09 May 2024 15:26:15 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/docs/References/</guid><description>References This page lists relevant papers, projects, and resources that have influenced the development of LLMIR or are related to LLM optimization and compilation.
Foundational Work MLIR Framework Chris Lattner, Mehdi Amini, Uday Bondhugula, Albert Cohen, Andy Davis, Jacques Pienaar, River Riddle, Tatiana Shpeisman, Nicolas Vasilache, and Oleksandr Zinenko. &amp;ldquo;MLIR: Scaling compiler infrastructure for domain specific computation.&amp;rdquo; In 2021 IEEE/ACM International Symposium on Code Generation and Optimization (CGO), pp. 2-14. IEEE, 2021.</description></item><item><title>FAQ</title><link>https://chenxingqiang.github.io/llmir-www/getting_started/Faq/</link><pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/getting_started/Faq/</guid><description>Frequently Asked Questions about LLMIR What is LLMIR? LLMIR (Large Language Model Intermediate Representation) is a compiler infrastructure for large language models based on MLIR (Multi-Level Intermediate Representation). It&amp;rsquo;s designed to optimize and accelerate LLM inference through specialized compilation techniques, providing a unified intermediate representation layer for different LLM frameworks.
What problem does LLMIR solve? LLMIR addresses several key challenges in LLM inference:
Performance bottlenecks: By applying compiler optimization techniques, LLMIR reduces inference latency and improves throughput.</description></item><item><title>Reporting Issues</title><link>https://chenxingqiang.github.io/llmir-www/getting_started/ReportingIssues/</link><pubDate>Wed, 27 Apr 2022 10:30:15 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/getting_started/ReportingIssues/</guid><description>Issues with MLIR can be reported through GitHub. Report the issue for the llvm-project repository at https://github.com/llvm/llvm-project/issues/new. If possible, attach the &amp;ldquo;mlir&amp;rdquo; label (label management may be limited to accounts that have a contribution history). Several other labels prefixed with &amp;ldquo;mlir:&amp;rdquo; are available if the issue can be classified further, for example, &amp;ldquo;mlir:core&amp;rdquo; can be used for issues with MLIR core libraries (mlir/lib/IR, mlir/lib/Interfaces, etc.) and &amp;ldquo;mlir:affine&amp;rdquo; can be used for issues with MLIR Affine dialect.</description></item><item><title>Debugging Tips</title><link>https://chenxingqiang.github.io/llmir-www/getting_started/Debugging/</link><pubDate>Mon, 30 Mar 2020 00:00:00 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/getting_started/Debugging/</guid><description>Inspecting compilation There&amp;rsquo;s no silver bullet for debugging the compilation process. Standard debugging techniques (printf debugging, gdb/lldb, IDE graphical debuggers, etc.) are of course applicable, but below are MLIR-specific facilities that are quite useful before diving into a generic debug flow. These facilities assume that you have reduced your problem to a form that can be reproduced with mlir-opt or another program that hooks into MLIR&amp;rsquo;s option parsing, if this is not the case, see section &amp;ldquo;Isolating test case&amp;rdquo; below.</description></item><item><title>How to Contribute</title><link>https://chenxingqiang.github.io/llmir-www/getting_started/Contributing/</link><pubDate>Fri, 29 Nov 2019 15:26:15 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/getting_started/Contributing/</guid><description>Everyone is welcome to contribute to MLIR. There are several ways of getting involved and contributing including reporting bugs, improving documentation and tutorials.
Community Guidelines Please be mindful of the LLVM Code of Conduct, which pledges to foster an open and welcoming environment.
Contributing code Please send pull-request on GitHub. If you don&amp;rsquo;t have write access to the repo, just leave a comment asking the reviewer to hit the merge button it for you.</description></item><item><title>Open Projects</title><link>https://chenxingqiang.github.io/llmir-www/getting_started/openprojects/</link><pubDate>Fri, 29 Nov 2019 15:26:15 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/getting_started/openprojects/</guid><description>Below is a list of projects that can be suitable for Google Summer of Code (GSOC) or just for someone to get started with contributing to MLIR. See also the &amp;ldquo;beginner&amp;rdquo; issues on the bugtracker. If you&amp;rsquo;re interested in one of these projects, feel free to discuss it on the MLIR section of the LLVM forums or on the MLIR channel of the LLVM discord server. The mentors are indicative and suggestion of first point of contact for starting on these projects.</description></item><item><title>Glossary</title><link>https://chenxingqiang.github.io/llmir-www/getting_started/Glossary/</link><pubDate>Fri, 29 Nov 2019 15:26:15 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/getting_started/Glossary/</guid><description>This glossary contains definitions of MLIR-specific terminology. It is intended to be a quick reference document. For terms which are well-documented elsewhere, definitions are kept brief and the header links to the more in-depth documentation.
Block A sequential list of operations without control flow.
Also called a basic block.
Conversion The transformation of code represented in one dialect into a semantically equivalent representation in another dialect (i.e. inter-dialect conversion) or the same dialect (i.</description></item><item><title>Testing Guide</title><link>https://chenxingqiang.github.io/llmir-www/getting_started/TestingGuide/</link><pubDate>Fri, 29 Nov 2019 15:26:15 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/getting_started/TestingGuide/</guid><description>Quickstart commands Run all MLIR tests: Run integration tests (requires -DMLIR_INCLUDE_INTEGRATION_TESTS=ON): Run C++ unit tests: Run lit tests in a specific directory Run a specific lit test file Test categories lit and FileCheck tests Diagnostic tests Integration tests C++ Unit tests Contributor guidelines FileCheck best practices Test Formatting Best Practices Test Documentation Best Practices Quickstart commands These commands are explained below in more detail. All commands are run from the cmake build directory build/, after building the project.</description></item><item><title>Developer Guide</title><link>https://chenxingqiang.github.io/llmir-www/getting_started/DeveloperGuide/</link><pubDate>Thu, 09 May 2024 15:26:15 +0000</pubDate><guid>https://chenxingqiang.github.io/llmir-www/getting_started/DeveloperGuide/</guid><description>LLMIR Developer Guide This guide provides an overview of how to develop with LLMIR.
Building LLMIR LLMIR is built on top of the MLIR ecosystem. To build LLMIR, you&amp;rsquo;ll need:
A C++ compiler (GCC or Clang) with C++17 support CMake (3.13.4 or higher) Python (3.7 or higher) Ninja or Make build system Clone the Repository git clone https://github.com/chenxingqiang/llmir.git cd llmir Configure the Build mkdir build &amp;amp;&amp;amp; cd build cmake -G Ninja .</description></item></channel></rss>